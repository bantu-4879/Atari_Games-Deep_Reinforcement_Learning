{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Libraries\n",
    "import gymnasium as gym\n",
    "import torch\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.monitor import Monitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment - Lunar Lander V2\n",
    "env = gym.make(\"LunarLander-v2\")\n",
    "observation, info = env.reset()\n",
    "\n",
    "print(\"Observation Space Shape: \", env.observation_space.shape)\n",
    "print(\"Action Space Shape: \",env.action_space.n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Understand the Environment</h1>\n",
    "\n",
    "<h2>Action Space</h2>\n",
    "<p>There are four discrete actions available:</p>\n",
    "<ul class=\"simple\">\n",
    "<li><p>0: do nothing</p></li>\n",
    "<li><p>1: fire left orientation engine</p></li>\n",
    "<li><p>2: fire main engine</p></li>\n",
    "<li><p>3: fire right orientation engine</p></li>\n",
    "</ul>\n",
    "\n",
    "<h2>Observation Space</h2>\n",
    "<p>There are eight different variables available:</p>\n",
    "<ul>\n",
    "<li><p>Horizontal pad coordinate (x)</p></li>\n",
    "<li>Vertical pad coordinate (y)</p></li>\n",
    "<li><p>Horizontal speed (x)</p></li>\n",
    "<li><p>Vertical speed (y)</p></li>\n",
    "<li><p>Angle</p></li>\n",
    "<li>Vertical pad coordinate (y)</p></li>\n",
    "<li><p>If the left leg contact point has touched the land (boolean)</p></li>\n",
    "<li><p>If the right leg contact point has touched the land (boolean)</p></li>\n",
    "</ul>\n",
    "\n",
    "<h2>Rewards</h2>\n",
    "<p>After every step a reward is granted. The total reward of an episode is the sum of the rewards for all the steps within that episode.</p>\n",
    "<p>For each step, the reward:</p>\n",
    "<ul>\n",
    "<li><p>is increased/decreased the closer/further the lander is to the landing pad</p></li>\n",
    "<li><p>is increased/decreased the slower/faster the lander is moving.</p></li>\n",
    "<li><p>is decreased the more the lander is tilted (angle not horizontal)</p></li>\n",
    "<li><p>is increased by 10 points for each leg that is in contact with the ground</p></li>\n",
    "<li><p>is decreased by 0.03 points each frame a side engine is firing.</p></li>\n",
    "<li><p>is decreased by 0.3 points each frame the main engine is firing</p></li>\n",
    "</ul>\n",
    "\n",
    "<p>The episode receive an additional reward of -100 or +100 points for crashing or landing safely respectively.</p>\n",
    "<p>An episode is considered a solution if it scores at least 200 points. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a vectorized environment - a method for stacking multiple independent environments into a single environment\n",
    "env = make_vec_env(\"LunarLander-v2\", n_envs=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "model = PPO(\n",
    "    policy = 'MlpPolicy',\n",
    "    env = env,\n",
    "    learning_rate=0.001,\n",
    "    n_steps = 1024,\n",
    "    batch_size = 64,\n",
    "    n_epochs = 5,\n",
    "    gamma = 0.999,\n",
    "    gae_lambda = 0.98,\n",
    "    ent_coef = 0.01,\n",
    "    verbose=1,\n",
    "    vf_coef= 0.5, \n",
    "    max_grad_norm= 0.5,\n",
    "    device='mps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the PPO Agent\n",
    "model.learn(total_timesteps=2500000) # 2.5 million timesteps\n",
    "\n",
    "# save the model\n",
    "model_name = 'PPO-LunarLander-v2'\n",
    "model.save(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "eval_env = Monitor(gym.make(\"LunarLander-v2\"))\n",
    "mean_reward, std_reward = evaluate_policy(model, eval_env, n_eval_episodes=50, deterministic=True)\n",
    "print(f\"mean_reward={mean_reward:.2f} +/- {std_reward}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
